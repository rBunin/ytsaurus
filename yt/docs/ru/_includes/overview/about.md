# Обзор

В данном разделе можно узнать об основных возможностях и устройстве системы {{product-name}}.

## Кластеры { #clusters }

Кластер {{product-name}} — группа инстансов, расположенных в контейнерах на физических хостах и функционирующих как единое целое.
Исходя из решаемых задач, кластеры могут делиться на: 
- вычислительные кластеры для массивной параллельной обработки больших данных при помощи MapReduce операций;
- кластеры для динамических таблиц (key-value хранилище);
- геораспределённые кластеры, выполняющие функции координации.

Вычислительный кластер {{product-name}} способен:
- хранить эксабайты данных;
- использовать HDD, SSD и RAM для хранения данных;
- обрабатывать данные на сотнях тысяч Hyper Threading ядер;
- решать задачи машинного обучения на тысячах графических процессоров (GPU);
- хранить и обрабатывать данные десятков тысяч пользователей.

## Архитектура 

![](../../../images/architecture.png)

В архитектуре {{product-name}} можно выделить три слоя:
- распределённая файловая система и хранилище метаинформации — Кипарис;
- планировщик для распределённых вычислений с поддержкой модели MapReduce;
- высокоуровневые вычислительные движки: YQL, CHYT, SPYT.

### Кипарис { #cypress }

Кипарис — отказоустойчивое древовидное хранилище. Возможности Кипариса можно описать следующим образом:
- древовидное пространство имён, узлами которого являются директории, таблицы и файлы;
- прозрачное шардирование больших табличных данных на чанки, которое позволяет работать с таблицей, как единой сущностью, не думая о деталях её хранения;
- поддержка колоночного и строчного механизмов хранения табличных данных;
- поддержка сжатого хранения с использованием различных кодеков — например, lz4 и zstd с настраиваемым уровнем сжатия;
- поддержка erasure-кодирования с использованием разнообразных стратегий подсчета контрольных сумм, обладающих разными параметрами избыточности и допустимых видов потерь;
- выразительная схематизация данных с поддержкой иерархических типов и признаков сортированности данных;
- фоновые репликация и починка erasure-данных, не требующие ручных действий;
- транзакционная семантика с поддержкой вложенных транзакций и блокировок уровней snapshot/shared/exclusive;
- транзакции, которые могут затрагивать много объектов Кипариса и длиться неограниченно долго;
- гибкая система разделения и учёта дискового пространства;
- гибкая система разграничения прав доступа к данным, включая доступ к колонкам таблиц.

Основа Кипариса — реплицированный и горизонтально-масштабируемый мастер-сервер, который хранит метаинформацию об устройстве дерева Кипариса, а также о составе и местоположении реплик чанков всех таблиц на кластере. Мастер-серверы представляют собой Replicated State Machine, реализованную поверх технологии Hydra — in-house версии алгоритма консенсуса, схожей с Raft.
Кипарис реализует отказоустойчивый эластичный слой хранения данных.

Помимо хранения Кипарис может выступать сервисом координации (lock service), по аналогии с Apache Zookeeper. 

С пользовательской точки зрения Кипарис похож на дерево файловой системы в Linux. Самый простой способ работать с Кипарисом — использовать веб-интерфейс.

### Динамические таблицы { #dyn-tables }

Динамические таблицы — это вид таблиц в {{product-name}}, реализующих интерфейс точечного чтения и записи данных по ключу, поддерживающих транзакции и собственный диалект SQL. 

Ключевые особенности динамических таблиц:
- хранение данных в модели [MVCC](https://en.wikipedia.org/wiki/Multiversion_concurrency_control), позволяющее читать значения по ключу и таймстемпу;
- масштабируемость: динамическая таблица делится на таблеты (шарды по диапазонам ключей), которые обслуживаются отдельными узлами кластера;
- транзакционность: динамические таблицы — это OLTP-хранилища, позволяющие модифицировать в одной транзакции множество строк из разных таблетов различных таблиц;
- отказоустойчивость: выход отдельного узла кластера, обслуживающего таблет, приводит к тому, что данный таблет переезжает на другой узел кластера без потери данных;
- изоляция: инстансы, обслуживающие таблеты, группируются в бандлы, находящиеся на отдельных серверах, за счет чего обеспечивается изоляция нагрузки;
- проверка конфликтов на уровне отдельных ключей или даже отдельных значений;
- возможность ответа из RAM для горячих данных;
- удаление данных по [TTL](https://ru.wikipedia.org/wiki/Time_to_live);
- встроенный SQL-like язык для сканирующих аналитических запросов.

Помимо динамических таблиц с интерфейсом k-v storage в системе есть поддержка динамических таблиц, реализующих абстракцию очереди сообщений — а именно топика и потока. Указанные очереди также можно считать таблицами, потому что они состоят из строк и обладают собственной схемой. В одной транзакции можно одновременно изменять строки в k-v динамической таблице, а также в очереди. Это позволяет строить потоковую обработку данных поверх динамических таблиц {{product-name}} с exactly-once семантикой.

### MapReduce { #mapreduce }

В основе вычислительной архитектуры {{product-name}} лежит модель распределённых вычислений MapReduce. Операция Map обрабатывает входные данные, разделённые на части между узлами кластера, без обмена данными между узлами. Операция Reduce выполняет группировку данных с разных узлов кластера.
Модель позволяет обрабатывать большие объемы данных с высокой надежностью и автоматически перезапускать часть вычислений в случае недоступности отдельных узлов кластера.

MapReduce в {{product-name}} обладает следующими отличительными чертами:
- богатая модель базовых операций: классический MapReduce (с разными стратегиями shuffle и поддержкой многофазного партиционирования), Map, Erase, Sort, и разнообразные расширения модели с учётом сортированности входных данных;
- горизонтальная масштабируемость вычислительных операций: операции дробятся на джобы, которые работают на отдельных узлах кластера;
- поддержка до сотен тысяч джобов в одной операции;
- гибкая модель иерархических вычислительных пулов с мгновенными и интегральными гарантиями, а также fair_share распределением недоутилизированных ресурсов между потребителями без гарантий;
- векторная модель ресурсов, позволяющая заказывать различные вычислительные ресурсы (CPU, RAM, GPU) в разных пропорциях;
- запуск джобов на вычислительных узлах в контейнерах, изолированных друг от друга по CPU, RAM, файловой системе и process namespace с использованием механизма контейнеризации Porto;
- масштабирующийся планировщик, способный обслуживать кластеры с миллионом одновременно исполняемых задач;
- сохранение практически всего прогресса вычислений при обновлениях или выходе отдельных узлов планировщика из строя.

Следует отметить возможность запускать не только MapReduce-джобы, но и произвольный код. В терминологии {{product-name}} запуск произвольного кода достигается с помощью ванильных (Vanilla) операций.

### YQL { #yql }

YQL — универсальный, декларативный, основанный на SQL язык запросов к системам хранения и обработки данных, а также инфраструктура для их выполнения. 

К преимуществам YQL можно отнести:
- мощный графовый движок исполнения, который строит MapReduce-pipelines из сотен узлов и может адаптивно перестраиваться по ходу вычисления;
- возможность построения сложного конвейера обработки данных на SQL с сохранением подзапросов в переменные в виде цепочек зависимых запросов и транзакций;
- предсказуемое параллельное исполнение запросов произвольной сложности;
- эффективная реализация join-ов, подзапросов и оконных функций без ограничений на их топологию и вложенность;
- богатая библиотека функций;
- поддержка пользовательских функций на C++, Python и JavaScript;
- поддержка функций использования моделей машинного обучения с применением CatBoost и TensorFlow;
- автоматическое исполнение небольших частей запросов на заранее подготовленных вычислительных инстансах в обход MapReduce-операций для уменьшения времени выполнения.

### CHYT { #chyt }

ClickHouse over {{product-name}} (CHYT) — это технология, которая позволяет поднять кластер из серверов ClickHouse непосредственно на вычислительных узлах {{product-name}}.
ClickHouse поднимается внутри Vanilla-операции и работает с данными, которые находятся в {{product-name}}. Кластер {{product-name}} выступает compute-облаком по отношению к запущенным в нём кластерам CHYT.
Технология позволяет разным пользователям запустить на одном кластере {{product-name}} несколько кластеров CHYT, которые будут абсолютно изолированы друг от друга, и решить тем самым задачу разделения ресурсов в облачном стиле.

В экосистеме {{product-name}} CHYT отвечает:
- за быстрые аналитические запросы поверх статических таблиц в {{product-name}} со временем ответа менее секунды;
- переиспользование уже имеющихся данных в кластере {{product-name}} без необходимости копировать их в отдельный кластер ClickHouse;
- возможность интеграции через ODBC и JDBC-драйверы ClickHouse, например, со сторонними системами визуализации такими как: Tableau, Qlik, Power BI, Oracle Bi и DataLens.

Интеграция произведена на довольно низком уровне. Это позволяет получить максимум возможностей как от {{product-name}}, так от ClickHouse. В данной интеграции реализована:
- поддержка чтения как статических, так и динамических таблиц;
- частичная поддержка транзакционной модели {{product-name}};
- поддержка распределённых вставок;
- CPU-efficient-преобразование колоночных данных из внутреннего формата {{product-name}} в in-memory представление ClickHouse;
- агрессивное кэширование данных, позволяющее в ряде случаев читать данные для исполнения запросов исключительно из памяти инстансов.

### SPYT { #spyt }

SPYT — технология, интегрирующая Apache Spark в качестве вычислительного движка поверх данных в {{product-name}}. Как и в случае с CHYT, в качестве вычислительных ресурсов под кластер SPYT используются Vanilla операции {{product-name}}.
Использование SPYT позволяет минимизировать расходы на IO и достичь многократного роста производительности для сложных аналитических процессов обработки данных.
SPYT может читать как статические, так и динамические таблицы в {{product-name}}, выполнять расчёты на них и записывать результат в статическую таблицу {{product-name}}.

### SDK { #sdk }

Все API на популярных языках программирования (С++, Python, Java, Go) разработаны командой {{product-name}}, поэтому в них учтены и продуманы все тонкости взаимодействия с системой. Клиентская часть {{product-name}} для упомянутых языков позволяет записать или прочитать большой объем данных несмотря на возможные сетевые сбои и другие ошибки. 

### Веб-интерфейс { #ui }

В системе {{product-name}} реализован единый веб-интерфейс для пользователей и администраторов. С его помощью можно:
- перемещаться по Кипарису и просматривать данные;
- производить операции с таблицами, папками и файлами;
- запускать и просматривать MapReduce-вычисления;
- запускать и просматривать историю SQL-запросов во всех вычислительных движках — в YQL, CHYT, SQL динамических таблиц;
- администрировать систему: 
  - следить за состоянием компонент кластера;
  - создавать, удалять пользователей и группы;
  - управлять квотами;
  - просматривать настройки контроля доступа;
  - просматривать версии компонент кластера и многое другое.

