# Обзор

## Что такое Spark? { #what-is-spark }

[Apache Spark](https://spark.apache.org/) — это фреймворк для расчетов на больших данных (джойнов, группировок, фильтраций и т. д.).

Spark обрабатывает данные в оперативной памяти. Ключевое отличие процессинга в памяти от "классического" MapReduce образца 2005 года в том, что данные минимально затрагивают диск при работе, а значит, минимизируются расходы на IO — самую медленную часть процессинга. Для одиночной Map операции эффект от использования Spark не будет заметен. Но уже для одного каскада Map и Reduce удается избежать записи промежуточных результатов на диск при условии, что памяти будет достаточно.

Для каждого последующего каскада MapReduce экономия нарастает, появляется возможность кешировать результаты. Для больших и сложных аналитических пайплайнов рост производительности будет многократным.

Также Spark вооружен полноценным оптимизатором запросов [Catalyst](https://github.com/tupol/spark-catalyst-study/blob/master/docs/catalyst-description.md), который планирует выполнение и учитывает:
- расположение и объёмы входных данных;
- протягивание предикатов до файловой системы;
- целесообразность и порядок шагов при исполнении запроса;
- набор атрибутов в конечной таблице;
- локальность данных при обработке;
- возможную конвейеризацию вычислений.

## Как Spark интегрирован с {{product-name}}

Подробности интеграции Spark c {{product-name}} можно узнать в [вебинаре](https://youtu.be/gZ7m-2_LqB0).


